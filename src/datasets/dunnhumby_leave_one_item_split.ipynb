{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ta-Feng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/tr.zaiqm/mycode/tr_rec/configs/\n",
      "/users/tr.zaiqm/mycode/tr_rec/datasets/\n",
      "/users/tr.zaiqm/mycode/tr_rec/checkpoints/\n",
      "/users/tr.zaiqm/mycode/tr_rec/results/\n",
      "/users/tr.zaiqm/mycode/tr_rec/samples/\n",
      "/users/tr.zaiqm/mycode/tr_rec/logs/\n",
      "/users/tr.zaiqm/mycode/tr_rec/runs/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import random\n",
    "from pandas.core.frame import DataFrame\n",
    "import sklearn\n",
    "from utils.unigramTable import UnigramTable\n",
    "\n",
    "\n",
    "DEFAULT_USER_COL = \"user_ids\"\n",
    "DEFAULT_ITEM_COL = \"item_ids\"\n",
    "DEFAULT_ORDER_COL = \"order_ids\"\n",
    "DEFAULT_RATING_COL = \"ratings\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "DEFAULT_FLAG_COL = \"flag\"\n",
    "data_base_dir = \"../../datasets/tafeng/\"\n",
    "\n",
    "\n",
    "test_percents = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "# validate percent just choose the same percent in the training set\n",
    "\n",
    "negative_size = 100\n",
    "\n",
    "min_u_c = 10  # items which were purcharsed by at least min_u_c users\n",
    "min_i_c = 10  # users buy at least min_i_c items\n",
    "min_o_c = 10  ##users have at least min_o_c orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load full data to sequence DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepro_to_seq(data_base_dir):\n",
    "    #     if not os.path.isdir(data_base_dir + \"data\"):  # dir to output preprosessed data\n",
    "    #         print(\n",
    "    #             \"no data proprecessing dir found, creat dir to output the preprosessed data\"\n",
    "    #         )\n",
    "    #         os.makedirs(data_base_dir + \"data\")\n",
    "\n",
    "    #     print(\"start loading data from raw data\")\n",
    "\n",
    "    transaction_data = data_base_dir + \"transaction_data.csv\"\n",
    "    prior_transaction = pd.read_csv(\n",
    "        transaction_data,\n",
    "        usecols=[\"BASKET_ID\", \"household_key\", \"PRODUCT_ID\", \"DAY\", \"TRANS_TIME\"],\n",
    "    )\n",
    "\n",
    "    prior_transaction[\"DAY\"] = prior_transaction[\"DAY\"].astype(str)  #\n",
    "    prior_transaction[\"TRANS_TIME\"] = prior_transaction[\"TRANS_TIME\"].astype(str)\n",
    "\n",
    "    prior_transaction[\"time\"] = (\n",
    "        prior_transaction[\"DAY\"] + prior_transaction[\"TRANS_TIME\"]\n",
    "    )\n",
    "    prior_transaction[\"time\"] = prior_transaction[\"time\"].astype(int)  #\n",
    "    prior_transaction.reset_index(inplace=True)\n",
    "    prior_transaction = prior_transaction.sort_values(by=\"time\", ascending=False)\n",
    "\n",
    "    prior_transaction.drop([\"DAY\", \"TRANS_TIME\"], axis=1)\n",
    "\n",
    "    prior_transaction = prior_transaction[\n",
    "        [\"BASKET_ID\", \"household_key\", \"PRODUCT_ID\", \"time\"]\n",
    "    ]\n",
    "    prior_transaction.insert(3, \"flag\", \"train\")\n",
    "    prior_transaction.insert(4, \"ratings\", 1)\n",
    "    prior_transaction.rename(\n",
    "        columns={\n",
    "            \"BASKET_ID\": DEFAULT_ORDER_COL,\n",
    "            \"household_key\": DEFAULT_USER_COL,\n",
    "            \"PRODUCT_ID\": DEFAULT_ITEM_COL,\n",
    "            \"flag\": DEFAULT_FLAG_COL,\n",
    "            \"ratings\": DEFAULT_RATING_COL,\n",
    "            \"time\": DEFAULT_TIMESTAMP_COL,\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    print(\"loading raw data completed\")\n",
    "    return prior_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw data completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_ids</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>item_ids</th>\n",
       "      <th>flag</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2592448</td>\n",
       "      <td>42289544807</td>\n",
       "      <td>623</td>\n",
       "      <td>1008673</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591966</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>18038469</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591965</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>10149684</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591964</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>973016</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591963</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>934131</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591962</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>911974</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591961</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>878141</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591386</td>\n",
       "      <td>42289363311</td>\n",
       "      <td>1651</td>\n",
       "      <td>1114483</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591387</td>\n",
       "      <td>42289363311</td>\n",
       "      <td>1651</td>\n",
       "      <td>9526239</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2592825</td>\n",
       "      <td>42289604274</td>\n",
       "      <td>2122</td>\n",
       "      <td>949616</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112259</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           order_ids  user_ids  item_ids   flag  ratings  timestamp\n",
       "2592448  42289544807       623   1008673  train        1    7112343\n",
       "2591966  42289474243       820  18038469  train        1    7112334\n",
       "2591965  42289474243       820  10149684  train        1    7112334\n",
       "2591964  42289474243       820    973016  train        1    7112334\n",
       "2591963  42289474243       820    934131  train        1    7112334\n",
       "2591962  42289474243       820    911974  train        1    7112334\n",
       "2591961  42289474243       820    878141  train        1    7112334\n",
       "2591386  42289363311      1651   1114483  train        1    7112301\n",
       "2591387  42289363311      1651   9526239  train        1    7112301\n",
       "2592825  42289604274      2122    949616  train        1    7112259"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_base_dir = \"../../datasets/dunnhumby/\"\n",
    "full_data = data_prepro_to_seq(\n",
    "    data_base_dir + \"raw/dunnhumby_The-Complete-Journey/csv/\"\n",
    ")\n",
    "full_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_ids</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>item_ids</th>\n",
       "      <th>flag</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2592448</td>\n",
       "      <td>42289544807</td>\n",
       "      <td>623</td>\n",
       "      <td>1008673</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591966</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>18038469</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591965</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>10149684</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591964</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>973016</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2591963</td>\n",
       "      <td>42289474243</td>\n",
       "      <td>820</td>\n",
       "      <td>934131</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>7112334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           order_ids  user_ids  item_ids   flag  ratings  timestamp\n",
       "2592448  42289544807       623   1008673  train        1    7112343\n",
       "2591966  42289474243       820  18038469  train        1    7112334\n",
       "2591965  42289474243       820  10149684  train        1    7112334\n",
       "2591964  42289474243       820    973016  train        1    7112334\n",
       "2591963  42289474243       820    934131  train        1    7112334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row data staticstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2595732, 276484, 2500, 92339)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_interact = len(full_data.index)\n",
    "n_orders = full_data[DEFAULT_ORDER_COL].nunique()\n",
    "n_users = full_data[DEFAULT_USER_COL].nunique()\n",
    "n_items = full_data[DEFAULT_ITEM_COL].nunique()\n",
    "(n_interact, n_orders, n_users, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_ids    42305362535\n",
       "user_ids            2500\n",
       "item_ids        18316298\n",
       "flag               train\n",
       "ratings                1\n",
       "timestamp        7112343\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the integrity of the saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2595732, 276484, 2500, 92339)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_interact = len(full_data.index)\n",
    "n_orders = full_data[DEFAULT_ORDER_COL].nunique()\n",
    "n_users = full_data[DEFAULT_USER_COL].nunique()\n",
    "n_items = full_data[DEFAULT_ITEM_COL].nunique()\n",
    "(n_interact, n_orders, n_users, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_neg_sample(eval_df, negative_num, item_sampler):\n",
    "    print(\"sampling negative items...\")\n",
    "    interact_status = eval_df.groupby([\"user_ids\"])[\"item_ids\"].apply(set).reset_index()\n",
    "    total_interact = pd.DataFrame(\n",
    "        {\"user_ids\": [], \"item_ids\": [], \"ratings\": []}, dtype=np.long\n",
    "    )\n",
    "    for index, user_items in interact_status.iterrows():\n",
    "        u = int(user_items[\"user_ids\"])\n",
    "        items = set(user_items[\"item_ids\"])  # item set for user u\n",
    "        n_items = len(items)  # number of positive item for user u\n",
    "        sample_neg_items = set(\n",
    "            item_sampler.sample(negative_num + n_items, 1, True)\n",
    "        )  # first sample negative_num+n_items items\n",
    "        sample_neg_items = list(sample_neg_items - items)[:negative_num]\n",
    "        # filter the positive items and truncate the first negative_num\n",
    "        #     print(len(sample_neg_items))\n",
    "        tp_items = np.append(list(items), sample_neg_items)\n",
    "        #     print(len(tp_items))\n",
    "\n",
    "        tp_users = np.array([1] * (negative_num + n_items), dtype=np.long) * u\n",
    "        tp_ones = np.ones(n_items, dtype=np.long)\n",
    "        tp_zeros = np.zeros(negative_num, dtype=np.long)\n",
    "        ratings = np.append(tp_ones, tp_zeros)\n",
    "        #     print(len(tp_users)),print(len(tp_items)),print(len(ratings))\n",
    "        tp = pd.DataFrame(\n",
    "            {\"user_ids\": tp_users, \"item_ids\": tp_items, \"ratings\": ratings}\n",
    "        )\n",
    "        total_interact = total_interact.append(tp)\n",
    "\n",
    "    total_interact = sklearn.utils.shuffle(total_interact)\n",
    "    return total_interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  divide data into train, validata and test sets, where validata set is in the train set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_valid_by_orders(seq_data, validate_size=0.2, test_size=0.2):\n",
    "    print(\"split_test_valid_by_orders\")\n",
    "    seq_data[\"flag\"] == \"train\"\n",
    "    orders = seq_data[DEFAULT_ORDER_COL].unique()\n",
    "    total_size = len(orders)\n",
    "    validate_size = int(total_size * validate_size)\n",
    "    test_size = int(total_size * test_size)\n",
    "    np.sort(orders)\n",
    "    seq_data.loc[\n",
    "        seq_data[DEFAULT_ORDER_COL].isin(orders[total_size - test_size :]), \"flag\"\n",
    "    ] = \"test\"  # the last 20% of the total orders to be the test set\n",
    "    seq_data.loc[\n",
    "        seq_data[DEFAULT_ORDER_COL].isin(orders[: total_size - test_size]), \"flag\"\n",
    "    ] = \"train\"  # the other 80% of the total orders to be the test set\n",
    "    #     np.random.shuffle(orders[:validate_size])\n",
    "    # the last 20% of the training orders to be the validating set\n",
    "    unique_user_ids_test = seq_data[seq_data[\"flag\"] == \"test\"][\n",
    "        DEFAULT_USER_COL\n",
    "    ].unique()\n",
    "    unique_user_ids_train = seq_data[seq_data[\"flag\"] != \"test\"][\n",
    "        DEFAULT_USER_COL\n",
    "    ].unique()\n",
    "    unique_item_ids_test = seq_data[seq_data[\"flag\"] == \"test\"][\n",
    "        DEFAULT_ITEM_COL\n",
    "    ].unique()\n",
    "    unique_item_ids_train = seq_data[seq_data[\"flag\"] != \"test\"][\n",
    "        DEFAULT_ITEM_COL\n",
    "    ].unique()\n",
    "\n",
    "    seq_data.loc[\n",
    "        seq_data[DEFAULT_ORDER_COL].isin(\n",
    "            orders[total_size - test_size - validate_size : total_size - test_size]\n",
    "        ),\n",
    "        \"flag\",\n",
    "    ] = \"validate\"\n",
    "    # seq_data.drop('time', axis = 1, inplace = True)\n",
    "    print(\"labeling train validate test dataset finished \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter data by count of users, items and orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by group_col and filter filter_col that has less num unique() count\n",
    "def fiter_by_count(tp, group_col, filter_col, num):\n",
    "    ordercount = (\n",
    "        tp.groupby([group_col])[filter_col].nunique().rename(\"count\").reset_index()\n",
    "    )\n",
    "    filter_tp = tp[\n",
    "        tp[group_col].isin(ordercount[ordercount[\"count\"] >= num][group_col])\n",
    "    ]\n",
    "    return filter_tp\n",
    "\n",
    "\n",
    "# filter data by the minimum purcharce number of items and users\n",
    "def filter_triplets(tp, min_u_c=5, min_i_c=5, min_o_c=5):\n",
    "    print(\"filter data by the minimum purcharce number of items and users and orders\")\n",
    "    n_interact = len(tp.index)\n",
    "    n_orders = tp[DEFAULT_ORDER_COL].nunique()\n",
    "    n_users = tp[DEFAULT_USER_COL].nunique()\n",
    "    n_items = tp[DEFAULT_ITEM_COL].nunique()\n",
    "    print(\"before filter\", n_interact, n_orders, n_users, n_items)\n",
    "    # Filter users by mixmum number of orders\n",
    "    if min_o_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_USER_COL, DEFAULT_ORDER_COL, min_o_c)\n",
    "\n",
    "    # Filter users by mixmum number of items\n",
    "    if min_i_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_USER_COL, DEFAULT_ITEM_COL, min_i_c)\n",
    "\n",
    "    # Filter items by mixmum number of users\n",
    "    if min_u_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_ITEM_COL, DEFAULT_USER_COL, min_u_c)\n",
    "\n",
    "    n_interact = len(tp.index)\n",
    "    n_orders = tp[DEFAULT_ORDER_COL].nunique()\n",
    "    n_users = tp[DEFAULT_USER_COL].nunique()\n",
    "    n_items = tp[DEFAULT_ITEM_COL].nunique()\n",
    "    print(\"after filter\", n_interact, n_orders, n_users, n_items)\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    # usercount, itemcount = get_count(tp, 'user_ids'), get_count(tp, 'item_ids')\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_to_files(data, data_base_dir):\n",
    "\n",
    "    user_ids = data[DEFAULT_USER_COL].to_numpy(dtype=np.long)\n",
    "    item_ids = data[DEFAULT_ITEM_COL].to_numpy(dtype=np.long)\n",
    "    order_ids = data[DEFAULT_ORDER_COL].to_numpy(dtype=np.long)\n",
    "    timestamps = data[DEFAULT_TIMESTAMP_COL].to_numpy(dtype=np.long)\n",
    "    ratings = data[DEFAULT_RATING_COL].to_numpy(dtype=np.float32)\n",
    "    data_file = os.path.join(data_base_dir, \"leave_one_item\")\n",
    "    if not os.path.exists(data_file):\n",
    "        os.makedirs(data_file)\n",
    "    data_file = os.path.join(data_file, \"train.npz\")\n",
    "    np.savez_compressed(\n",
    "        data_file,\n",
    "        user_ids=user_ids,\n",
    "        item_ids=item_ids,\n",
    "        order_ids=order_ids,\n",
    "        timestamp=timestamps,\n",
    "        ratings=ratings,\n",
    "    )\n",
    "    print(\n",
    "        \"Data saving to file:\",\n",
    "        data_base_dir,\n",
    "        \"max_item_num:\",\n",
    "        np.max(item_ids),\n",
    "        \"max_user_num:\",\n",
    "        np.max(user_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_to_files(data, data_base_dir, suffix):\n",
    "    user_ids = data[DEFAULT_USER_COL].to_numpy(dtype=np.long)\n",
    "    item_ids = data[DEFAULT_ITEM_COL].to_numpy(dtype=np.long)\n",
    "    ratings = data[DEFAULT_RATING_COL].to_numpy(dtype=np.float32)\n",
    "    data_file = os.path.join(data_base_dir, \"leave_one_item\")\n",
    "    if not os.path.exists(data_file):\n",
    "        os.makedirs(data_file)\n",
    "    data_file = os.path.join(data_file, suffix)\n",
    "    np.savez_compressed(\n",
    "        data_file, user_ids=user_ids, item_ids=item_ids, ratings=ratings,\n",
    "    )\n",
    "    print(\n",
    "        \"Data saving to file:\",\n",
    "        data_base_dir,\n",
    "        \"max_item_num:\",\n",
    "        np.max(item_ids),\n",
    "        \"max_user_num:\",\n",
    "        np.max(user_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiter_full_data = fiter_by_count(full_data, DEFAULT_ITEM_COL, DEFAULT_USER_COL, 10)\n",
    "fiter_full_data = fiter_by_count(fiter_full_data, DEFAULT_USER_COL, DEFAULT_ITEM_COL, 3)\n",
    "users = fiter_full_data[DEFAULT_USER_COL].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2492/2492 [18:27<00:00,  2.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for user in tqdm(users):\n",
    "    top_2_item_index = (\n",
    "        fiter_full_data[fiter_full_data[DEFAULT_USER_COL] == user]\n",
    "        .sort_values(by=[DEFAULT_TIMESTAMP_COL], ascending=False)\n",
    "        .head(2)\n",
    "        .index\n",
    "    )\n",
    "    fiter_full_data.loc[top_2_item_index[0], [DEFAULT_FLAG_COL]] = \"test\"\n",
    "    fiter_full_data.loc[top_2_item_index[1], [DEFAULT_FLAG_COL]] = \"validate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling negatives and saving\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Filling unigram table\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n",
      "Data saving to file: ../../datasets/dunnhumby/ max_item_num: 18203921 max_user_num: 2500\n"
     ]
    }
   ],
   "source": [
    "print(\"sampling negatives and saving\")\n",
    "data_base_dir = \"../../datasets/dunnhumby/\"\n",
    "\n",
    "tp_train = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"train\"]\n",
    "tp_validate = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"validate\"]\n",
    "tp_test = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"test\"]\n",
    "\n",
    "save_train_to_files(tp_train, data_base_dir)\n",
    "item_sampler = UnigramTable(tp_train[DEFAULT_ITEM_COL].value_counts().to_dict())\n",
    "for i in range(10):\n",
    "    tp_validate_new = feed_neg_sample(tp_validate, 100, item_sampler)\n",
    "    tp_test_new = feed_neg_sample(tp_test, 100, item_sampler)\n",
    "    save_test_to_files(tp_validate_new, data_base_dir, suffix=\"valid\" + \"_\" + str(i))\n",
    "    save_test_to_files(tp_test_new, data_base_dir, suffix=\"test\" + \"_\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2379184, 2492, 2492)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tp_train.index),len(tp_validate.index),len(tp_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'DEFAULT_USER_COL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-98d1243423ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_USER_COL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_ITEM_COL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/users/tr.zaiqm/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'DEFAULT_USER_COL'"
     ]
    }
   ],
   "source": [
    "tp_train.DEFAULT_USER_COL.nunique(),tp_train.DEFAULT_ITEM_COL.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_ids    266060\n",
       "user_ids       2492\n",
       "item_ids      23404\n",
       "flag              1\n",
       "ratings           1\n",
       "timestamp    212549\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
